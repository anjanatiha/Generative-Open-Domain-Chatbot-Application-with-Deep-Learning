1.0
source ~/tensorflow1/bin/activate

1.6:
source ~/tensorflow/bin/activate 
cd /home/anjanatiha/Downloads/project/Code/current_code/nmt

rm -r /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model
mkdir /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model

python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/vocab  \
    --train_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/train \
    --dev_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2012  \
    --test_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2013 \
    --out_dir=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu



rm -r /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model
mkdir /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model

python -m nmt.nmt \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/vocab  \
    --train_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/train \
    --dev_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2012  \
    --test_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2013 \
    --out_dir=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --encoder_type=bi \
    --optimizer=adam \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu


rm -r /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model
mkdir /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model

python -m nmt.nmt \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/vocab  \
    --train_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/train \
    --dev_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2012  \
    --test_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2013 \
    --out_dir=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --encoder_type=bi \
    --optimizer=adam \
    --num_layers=2 \
    --num_units=128 \
    --dropout=0.2 \
    --metrics=bleu




python -m nmt.nmt \
    --attention=scaled_luong \
    --src=vi --tgt=en \
    --vocab_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/vocab  \
    --train_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/train \
    --dev_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2012  \
    --test_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2013 \
    --out_dir=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model \
    --num_train_steps=1000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=128 \
  
  --dropout=0.2 \
    --metrics=bleu


########################################old#######################################################################



source ~/tensorflow/bin/activate 
cd /home/anjanatiha/Downloads/project/Code/current_code/nmt

tensorboard --port 22222 --logdir /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model




source ~/tensorflow/bin/activate 
cd /home/anjanatiha/Downloads/project/Code/current_code/nmt

cat > /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model/my_infer_file.vi


python -m nmt.nmt \
    --out_dir=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model \
    --inference_input_file=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model/my_infer_file.vi \
    --inference_output_file=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model/output_infer


cat /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model/output_infer




cat > /home/anjanatiha/AnacondaProjects/Masters/project/current_code/inout/output/nmt_attention_model/my_infer_file.vi


python -m nmt.nmt \
    --out_dir=/home/anjanatiha/AnacondaProjects/Masters/project/current_code/inout/output/nmt_attention_model \
    --inference_input_file=/home/anjanatiha/AnacondaProjects/Masters/project/current_code/inout/output/my_infer_file.vi \
    --inference_output_file=/home/anjanatiha/AnacondaProjects/Masters/project/current_code/inout/output/nmt_attention_model/output_infer



cat /home/anjanatiha/AnacondaProjects/Masters/project/current_code/inout/output/nmt_attention_model/output_infer




# sendex params


source ~/tensorflow/bin/activate 
cd /home/anjanatiha/Downloads/project/Code/current_code/nmt

rm -r /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model
mkdir /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model

python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/vocab  \
    --train_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/train \
    --dev_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2012  \
    --test_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2013 \
    --out_dir=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model \
    --attention=scaled_luong \
    --num_train_steps=12000 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=512 \
    --learning_rate=0.001 \
    --decay_steps=1 \
    --start_decay_step=1 \
    --beam_width=10 \
    --length_penalty_weight=1.0 \
    --optimizer=adam \
    --encoder_type=bi \
    --num_translations_per_input=30





source ~/tensorflow/bin/activate 
cd /home/anjanatiha/Downloads/project/Code/current_code/nmt

rm -r /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model
mkdir /home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model

python -m nmt.nmt \
    --src=vi --tgt=en \
    --vocab_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/vocab  \
    --train_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/train \
    --dev_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2012  \
    --test_prefix=/home/anjanatiha/Downloads/project/Code/current_code/inout/input/nmt_data/tst2013 \
    --out_dir=/home/anjanatiha/Downloads/project/Code/current_code/inout/output/nmt_model \
    --attention=scaled_luong \
    --num_train_steps=300 \
    --steps_per_stats=100 \
    --num_layers=2 \
    --num_units=512 \
    --learning_rate=0.001 \
    --decay_steps=1 \
    --start_decay_step=1 \
    --beam_width=10 \
    --length_penalty_weight=1.0 \
    --optimizer=adam \
    --encoder_type=bi \
    --num_translations_per_input=30











hparams = {
    'attention': 'scaled_luong',
    'src': 'from',
    'tgt': 'to',
    'vocab_prefix': os.path.join(train_dir, "vocab"),
    'train_prefix': os.path.join(train_dir, "train"),
    'dev_prefix': os.path.join(train_dir, "tst2012"),
    'test_prefix': os.path.join(train_dir, "tst2013"),
    'out_dir': out_dir,
    'num_train_steps': 500000,
    'num_layers': 2,
    'num_units': 512,
    'override_loaded_hparams': True,
    'learning_rate':0.001,
#    'decay_factor': 0.99998,
    'decay_steps': 1,
#    'residual': True,
    'start_decay_step': 1,
    'beam_width': 10,
    'length_penalty_weight': 1.0,
    'optimizer': 'adam',
    'encoder_type': 'bi',
    'num_translations_per_input': 30
}




def add_arguments(parser):
  """Build ArgumentParser."""
  parser.register("type", "bool", lambda v: v.lower() == "true")

  # network
  parser.add_argument("--num_units", type=int, default=32, help="Network size.")
  parser.add_argument("--num_layers", type=int, default=2,
                      help="Network depth.")
  parser.add_argument("--num_encoder_layers", type=int, default=None,
                      help="Encoder depth, equal to num_layers if None.")
  parser.add_argument("--num_decoder_layers", type=int, default=None,
                      help="Decoder depth, equal to num_layers if None.")
  parser.add_argument("--encoder_type", type=str, default="uni", help="""\
      uni | bi | gnmt.
      For bi, we build num_encoder_layers/2 bi-directional layers.
      For gnmt, we build 1 bi-directional layer, and (num_encoder_layers - 1)
        uni-directional layers.\
      """)
  parser.add_argument("--residual", type="bool", nargs="?", const=True,
                      default=False,
                      help="Whether to add residual connections.")
  parser.add_argument("--time_major", type="bool", nargs="?", const=True,
                      default=True,
                      help="Whether to use time-major mode for dynamic RNN.")
  parser.add_argument("--num_embeddings_partitions", type=int, default=0,
                      help="Number of partitions for embedding vars.")

  # attention mechanisms
  parser.add_argument("--attention", type=str, default="", help="""\
      luong | scaled_luong | bahdanau | normed_bahdanau or set to "" for no
      attention\
      """)
  parser.add_argument(
      "--attention_architecture",
      type=str,
      default="standard",
      help="""\
      standard | gnmt | gnmt_v2.
      standard: use top layer to compute attention.
      gnmt: GNMT style of computing attention, use previous bottom layer to
          compute attention.
      gnmt_v2: similar to gnmt, but use current bottom layer to compute
          attention.\
      """)
  parser.add_argument(
      "--output_attention", type="bool", nargs="?", const=True,
      default=True,
      help="""\
      Only used in standard attention_architecture. Whether use attention as
      the cell output at each timestep.
      .\
      """)
  parser.add_argument(
      "--pass_hidden_state", type="bool", nargs="?", const=True,
      default=True,
      help="""\
      Whether to pass encoder's hidden state to decoder when using an attention
      based model.\
      """)

  # optimizer
  parser.add_argument("--optimizer", type=str, default="sgd", help="sgd | adam")
  parser.add_argument("--learning_rate", type=float, default=1.0,
                      help="Learning rate. Adam: 0.001 | 0.0001")
  parser.add_argument("--warmup_steps", type=int, default=0,
                      help="How many steps we inverse-decay learning.")
  parser.add_argument("--warmup_scheme", type=str, default="t2t", help="""\
      How to warmup learning rates. Options include:
        t2t: Tensor2Tensor's way, start with lr 100 times smaller, then
             exponentiate until the specified lr.\
      """)
  parser.add_argument(
      "--decay_scheme", type=str, default="", help="""\
      How we decay learning rate. Options include:
        luong234: after 2/3 num train steps, we start halving the learning rate
          for 4 times before finishing.
        luong5: after 1/2 num train steps, we start halving the learning rate
          for 5 times before finishing.\
        luong10: after 1/2 num train steps, we start halving the learning rate
          for 10 times before finishing.\
      """)

  parser.add_argument(
      "--num_train_steps", type=int, default=12000, help="Num steps to train.")
  parser.add_argument("--colocate_gradients_with_ops", type="bool", nargs="?",
                      const=True,
                      default=True,
                      help=("Whether try colocating gradients with "
                            "corresponding op"))

  # initializer
  parser.add_argument("--init_op", type=str, default="uniform",
                      help="uniform | glorot_normal | glorot_uniform")
  parser.add_argument("--init_weight", type=float, default=0.1,
                      help=("for uniform init_op, initialize weights "
                            "between [-this, this]."))

  # data
  parser.add_argument("--src", type=str, default=None,
                      help="Source suffix, e.g., en.")
  parser.add_argument("--tgt", type=str, default=None,
                      help="Target suffix, e.g., de.")
  parser.add_argument("--train_prefix", type=str, default=None,
                      help="Train prefix, expect files with src/tgt suffixes.")
  parser.add_argument("--dev_prefix", type=str, default=None,
                      help="Dev prefix, expect files with src/tgt suffixes.")
  parser.add_argument("--test_prefix", type=str, default=None,
                      help="Test prefix, expect files with src/tgt suffixes.")
  parser.add_argument("--out_dir", type=str, default=None,
                      help="Store log/model files.")

  # Vocab
  parser.add_argument("--vocab_prefix", type=str, default=None, help="""\
      Vocab prefix, expect files with src/tgt suffixes.\
      """)
  parser.add_argument("--embed_prefix", type=str, default=None, help="""\
      Pretrained embedding prefix, expect files with src/tgt suffixes.
      The embedding files should be Glove formated txt files.\
      """)
  parser.add_argument("--sos", type=str, default="<s>",
                      help="Start-of-sentence symbol.")
  parser.add_argument("--eos", type=str, default="</s>",
                      help="End-of-sentence symbol.")
  parser.add_argument("--share_vocab", type="bool", nargs="?", const=True,
                      default=False,
                      help="""\
      Whether to use the source vocab and embeddings for both source and
      target.\
      """)
  parser.add_argument("--check_special_token", type="bool", default=True,
                      help="""\
                      Whether check special sos, eos, unk tokens exist in the
                      vocab files.\
                      """)

  # Sequence lengths
  parser.add_argument("--src_max_len", type=int, default=50,
                      help="Max length of src sequences during training.")
  parser.add_argument("--tgt_max_len", type=int, default=50,
                      help="Max length of tgt sequences during training.")
  parser.add_argument("--src_max_len_infer", type=int, default=None,
                      help="Max length of src sequences during inference.")
  parser.add_argument("--tgt_max_len_infer", type=int, default=None,
                      help="""\
      Max length of tgt sequences during inference.  Also use to restrict the
      maximum decoding length.\
      """)

  # Default settings works well (rarely need to change)
  parser.add_argument("--unit_type", type=str, default="lstm",
                      help="lstm | gru | layer_norm_lstm | nas")
  parser.add_argument("--forget_bias", type=float, default=1.0,
                      help="Forget bias for BasicLSTMCell.")
  parser.add_argument("--dropout", type=float, default=0.2,
                      help="Dropout rate (not keep_prob)")
  parser.add_argument("--max_gradient_norm", type=float, default=5.0,
                      help="Clip gradients to this norm.")
  parser.add_argument("--batch_size", type=int, default=128, help="Batch size.")

  parser.add_argument("--steps_per_stats", type=int, default=100,
                      help=("How many training steps to do per stats logging."
                            "Save checkpoint every 10x steps_per_stats"))
  parser.add_argument("--max_train", type=int, default=0,
                      help="Limit on the size of training data (0: no limit).")
  parser.add_argument("--num_buckets", type=int, default=5,
                      help="Put data into similar-length buckets.")

  # SPM
  parser.add_argument("--subword_option", type=str, default="",
                      choices=["", "bpe", "spm"],
                      help="""\
                      Set to bpe or spm to activate subword desegmentation.\
                      """)

  # Misc
  parser.add_argument("--num_gpus", type=int, default=1,
                      help="Number of gpus in each worker.")
  parser.add_argument("--log_device_placement", type="bool", nargs="?",
                      const=True, default=False, help="Debug GPU allocation.")
  parser.add_argument("--metrics", type=str, default="bleu",
                      help=("Comma-separated list of evaluations "
                            "metrics (bleu,rouge,accuracy)"))
  parser.add_argument("--steps_per_external_eval", type=int, default=None,
                      help="""\
      How many training steps to do per external evaluation.  Automatically set
      based on data if None.\
      """)
  parser.add_argument("--scope", type=str, default=None,
                      help="scope to put variables under")
  parser.add_argument("--hparams_path", type=str, default=None,
                      help=("Path to standard hparams json file that overrides"
                            "hparams values from FLAGS."))
  parser.add_argument("--random_seed", type=int, default=None,
                      help="Random seed (>0, set a specific seed).")
  parser.add_argument("--override_loaded_hparams", type="bool", nargs="?",
                      const=True, default=False,
                      help="Override loaded hparams with values specified")
  parser.add_argument("--num_keep_ckpts", type=int, default=5,
                      help="Max number of checkpoints to keep.")
  parser.add_argument("--avg_ckpts", type="bool", nargs="?",
                      const=True, default=False, help=("""\
                      Average the last N checkpoints for external evaluation.
                      N can be controlled by setting --num_keep_ckpts.\
                      """))

  # Inference
  parser.add_argument("--ckpt", type=str, default="",
                      help="Checkpoint file to load a model for inference.")
  parser.add_argument("--inference_input_file", type=str, default=None,
                      help="Set to the text to decode.")
  parser.add_argument("--inference_list", type=str, default=None,
                      help=("A comma-separated list of sentence indices "
                            "(0-based) to decode."))
  parser.add_argument("--infer_batch_size", type=int, default=32,
                      help="Batch size for inference mode.")
  parser.add_argument("--inference_output_file", type=str, default=None,
                      help="Output file to store decoding results.")
  parser.add_argument("--inference_ref_file", type=str, default=None,
                      help=("""\
      Reference file to compute evaluation scores (if provided).\
      """))
  parser.add_argument("--beam_width", type=int, default=0,
                      help=("""\
      beam width when using beam search decoder. If 0 (default), use standard
      decoder with greedy helper.\
      """))
  parser.add_argument("--length_penalty_weight", type=float, default=0.0,
                      help="Length penalty for beam search.")
  parser.add_argument("--sampling_temperature", type=float,
                      default=0.0,
                      help=("""\
      Softmax sampling temperature for inference decoding, 0.0 means greedy
      decoding. This option is ignored when using beam search.\
      """))
  parser.add_argument("--num_translations_per_input", type=int, default=1,
                      help=("""\
      Number of translations generated for each sentence. This is only used for
      inference.\
      """))

  # Job info
  parser.add_argument("--jobid", type=int, default=0,
                      help="Task id of the worker.")
  parser.add_argument("--num_workers", type=int, default=1,
                      help="Number of workers (inference only).")
  parser.add_argument("--num_inter_threads", type=int, default=0,
                      help="number of inter_op_parallelism_threads")
  parser.add_argument("--num_intra_threads", type=int, default=0,
                      help="number of intra_op_parallelism_threads")


