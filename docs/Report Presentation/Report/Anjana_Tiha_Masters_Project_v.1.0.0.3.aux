\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\babel@aux{english}{}
\@writefile{toc}{\contentsline {chapter}{\numberline {1}Introduction}{3}{chapter.1}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {1.1}{\ignorespaces Ref 1}}{3}{figure.1.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Related Works}{4}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Sequence to Sequence (Seq2Seq)}{4}{section.2.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Sequence to Sequence Model}}{4}{figure.2.1}}
\citation{jurafskyRL}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces Ref 7}}{5}{figure.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Google's Neural Machine Translation(GNMT)}{5}{section.2.2}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Deep Reinforcement Learning}{5}{section.2.3}}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Limitations}{7}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces Some undesirable chatbot replies (ref 5)}}{7}{figure.3.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {4}Deep Neural Network for Chatbot}{8}{chapter.4}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {4.1}Recurrent Neural Network}{8}{section.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.1}Recurrent Neural Network Architecture}{8}{subsection.4.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.1}{\ignorespaces Recurrent Neural Network}}{8}{figure.4.1}}
\citation{seq2seq}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1.2}Long-Short-Term-Memory(LSTM)}{9}{subsection.4.1.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {4.2}{\ignorespaces Long-Short-Term-Memory(LSTM)}}{9}{figure.4.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Architecture}{10}{chapter.5}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Google's Neural Machine Translation (GNMT)}{10}{section.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Sequence to Sequence (Seq2Seq) Architecture}{10}{subsection.5.1.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Encoder-decoder architecture – example of a general approach for NMT. An encoder converts a source sentence into a "meaning" vector which is passed through a decoder to produce a translation.}}{10}{figure.5.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Neural machine translation – example of a deep recurrent architecture proposed by for translating a source sentence "I am a student" into a target sentence "Je suis étudiant". Here, "$<s>$" marks the start of the decoding process while "$</s>$" tells the decoder to stop.}}{11}{figure.5.2}}
\citation{nmt}
\citation{nmt}
\@writefile{toc}{\contentsline {subsubsection}{Neural Attention Mechanism\cite  {nmt}}{12}{section*.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Attention visualization – example of the alignments between source and target sentences}}{12}{figure.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces  Attention mechanism – example of an attention-based NMT system.}}{12}{figure.5.4}}
\@writefile{toc}{\contentsline {chapter}{\numberline {6}Data}{13}{chapter.6}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {6.1}Data Collection}{13}{section.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1.1}Dataset 1: Cornell Movie Subtitle Corpus}{14}{subsection.6.1.1}}
\@writefile{toc}{\contentsline {section}{\numberline {6.2}Data Preprocessing}{14}{section.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2.1}Preprocessing of Cornell Movie Subtitle Corpus}{14}{subsection.6.2.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {7}Implementation Summary}{15}{chapter.7}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {7.1}{\ignorespaces Algorithm Details}}{15}{table.7.1}}
\@writefile{lot}{\contentsline {table}{\numberline {7.2}{\ignorespaces Front-end and Back-end}}{15}{table.7.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {8}Hardware Specification}{16}{chapter.8}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {8.1}{\ignorespaces Hardware Platform- Local machines (personal laptop)}}{16}{table.8.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {9}Experiment}{16}{chapter.9}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {section}{\numberline {9.1}Training}{16}{section.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.1}Training Dataset}{16}{subsection.9.1.1}}
\@writefile{lot}{\contentsline {table}{\numberline {9.1}{\ignorespaces Training Dataset}}{16}{table.9.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1.2}Training Model and Parameters}{16}{subsection.9.1.2}}
\@writefile{toc}{\contentsline {chapter}{\numberline {10}Result}{17}{chapter.10}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lot}{\contentsline {table}{\numberline {10.1}{\ignorespaces Evaluation}}{18}{table.10.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {11}Graphical Interface (GUI)}{18}{chapter.11}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{lof}{\contentsline {figure}{\numberline {11.1}{\ignorespaces Graphical Interface of Intelligent Chatbot}}{18}{figure.11.1}}
\@writefile{toc}{\contentsline {chapter}{\numberline {12}Challenges}{18}{chapter.12}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {13}Discussion}{19}{chapter.13}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {14}Future Work}{20}{chapter.14}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {15}Conclusion}{20}{chapter.15}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\bibcite{jurafskyRL}{1}
\bibcite{nmt}{2}
\bibcite{seq2seqtutorial}{3}
\bibcite{seq2seq}{4}
\bibcite{seq2seqgoogle}{5}
\bibcite{CornellMovieDialogs}{6}
\@writefile{toc}{\contentsline {chapter}{\numberline {16}References}{22}{chapter.16}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\@writefile{toc}{\contentsline {chapter}{\numberline {17}Image Credits}{23}{chapter.17}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
